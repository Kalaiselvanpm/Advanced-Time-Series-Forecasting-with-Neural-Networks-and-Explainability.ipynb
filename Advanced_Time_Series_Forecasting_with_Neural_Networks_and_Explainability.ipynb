{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrSM7v-qMFNo"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Advanced Time Series Forecasting with Neural Networks and Explainability\n",
        "Comprehensive implementation with LSTM/Transformer, Optuna optimization, and SHAP\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import optuna\n",
        "from optuna.pruners import MedianPruner\n",
        "\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "import shap\n",
        "\n",
        "# ============================================================================\n",
        "# PART 1: SYNTHETIC TIME SERIES DATA GENERATION\n",
        "# ============================================================================\n",
        "\n",
        "def generate_multivariate_timeseries(n_steps=2000, n_features=3, seed=42):\n",
        "    \"\"\"\n",
        "    Generate synthetic multivariate time series with:\n",
        "    - Trend component (linear)\n",
        "    - Multiple seasonalities (daily, weekly, monthly patterns)\n",
        "    - Noise components\n",
        "\n",
        "    Args:\n",
        "        n_steps: Total number of time steps\n",
        "        n_features: Number of multivariate features\n",
        "        seed: Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with synthetic time series\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    t = np.arange(n_steps)\n",
        "\n",
        "    # Initialize features dict\n",
        "    data = {}\n",
        "\n",
        "    for feat_idx in range(n_features):\n",
        "        # Trend component\n",
        "        trend = 0.01 * t + 50 * np.sin(2 * np.pi * t / 365)\n",
        "\n",
        "        # Primary seasonality (daily pattern, 7-step cycle)\n",
        "        daily_seasonality = 10 * np.sin(2 * np.pi * t / 7)\n",
        "\n",
        "        # Secondary seasonality (weekly pattern, 30-step cycle)\n",
        "        weekly_seasonality = 8 * np.sin(2 * np.pi * t / 30)\n",
        "\n",
        "        # Monthly seasonality (365-step cycle)\n",
        "        monthly_seasonality = 5 * np.cos(2 * np.pi * t / 365)\n",
        "\n",
        "        # Autoregressive component\n",
        "        ar_component = np.zeros(n_steps)\n",
        "        ar_component[0] = np.random.randn()\n",
        "        for i in range(1, n_steps):\n",
        "            ar_component[i] = 0.7 * ar_component[i-1] + 0.3 * ar_component[i-1] * np.random.randn()\n",
        "\n",
        "        # Noise\n",
        "        noise = np.random.normal(0, 2, n_steps)\n",
        "\n",
        "        # Combine all components\n",
        "        feature = (trend + daily_seasonality + weekly_seasonality +\n",
        "                   monthly_seasonality + ar_component + noise)\n",
        "\n",
        "        data[f'feature_{feat_idx}'] = feature\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df['timestamp'] = pd.date_range(start='2020-01-01', periods=n_steps, freq='H')\n",
        "    df = df.set_index('timestamp')\n",
        "\n",
        "    return df\n",
        "\n",
        "# ============================================================================\n",
        "# PART 2: DATA PREPROCESSING & SEQUENCE WINDOWING\n",
        "# ============================================================================\n",
        "\n",
        "class TimeSeriesPreprocessor:\n",
        "    \"\"\"Handle scaling, sequence generation, and train/val/test splitting\"\"\"\n",
        "\n",
        "    def __init__(self, lookback=24, forecast_horizon=6):\n",
        "        self.lookback = lookback\n",
        "        self.forecast_horizon = forecast_horizon\n",
        "        self.scalers = {}\n",
        "\n",
        "    def fit_scalers(self, df):\n",
        "        \"\"\"Fit StandardScaler for each feature\"\"\"\n",
        "        for col in df.columns:\n",
        "            scaler = StandardScaler()\n",
        "            scaler.fit(df[[col]].values)\n",
        "            self.scalers[col] = scaler\n",
        "        return self\n",
        "\n",
        "    def transform(self, df):\n",
        "        \"\"\"Apply fitted scalers\"\"\"\n",
        "        df_scaled = df.copy()\n",
        "        for col in df.columns:\n",
        "            df_scaled[col] = self.scalers[col].transform(df[[col]].values).flatten()\n",
        "        return df_scaled\n",
        "\n",
        "    def inverse_transform(self, df_scaled, feature_name):\n",
        "        \"\"\"Inverse transform to original scale\"\"\"\n",
        "        return self.scalers[feature_name].inverse_transform(df_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "    def create_sequences(self, df_scaled, target_col=0):\n",
        "        \"\"\"\n",
        "        Create overlapping sequences for supervised learning\n",
        "        Returns: (X, y) where X shape is (samples, lookback, n_features)\n",
        "                       and y shape is (samples, forecast_horizon)\n",
        "        \"\"\"\n",
        "        data = df_scaled.values\n",
        "        X, y = [], []\n",
        "\n",
        "        for i in range(len(data) - self.lookback - self.forecast_horizon + 1):\n",
        "            X.append(data[i:i + self.lookback, :])\n",
        "            y.append(data[i + self.lookback:i + self.lookback + self.forecast_horizon, target_col])\n",
        "\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "    def train_val_test_split(self, X, y, train_ratio=0.7, val_ratio=0.15):\n",
        "        \"\"\"Split into train/val/test maintaining temporal order\"\"\"\n",
        "        n = len(X)\n",
        "        train_idx = int(n * train_ratio)\n",
        "        val_idx = train_idx + int(n * val_ratio)\n",
        "\n",
        "        return (X[:train_idx], y[:train_idx]), \\\n",
        "               (X[train_idx:val_idx], y[train_idx:val_idx]), \\\n",
        "               (X[val_idx:], y[val_idx:])\n",
        "\n",
        "# ============================================================================\n",
        "# PART 3: DEEP LEARNING MODELS\n",
        "# ============================================================================\n",
        "\n",
        "class LSTMForecaster(nn.Module):\n",
        "    \"\"\"LSTM-based time series forecaster\"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout,\n",
        "                 forecast_horizon, bidirectional=False):\n",
        "        super(LSTMForecaster, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.forecast_horizon = forecast_horizon\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
        "                           dropout=dropout, batch_first=True,\n",
        "                           bidirectional=bidirectional)\n",
        "\n",
        "        lstm_output_size = hidden_size * (2 if bidirectional else 1)\n",
        "        self.fc1 = nn.Linear(lstm_output_size, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(128, forecast_horizon)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"x shape: (batch_size, seq_len, input_size)\"\"\"\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        last_hidden = lstm_out[:, -1, :]  # Use last timestep\n",
        "        x = self.fc1(last_hidden)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerForecaster(nn.Module):\n",
        "    \"\"\"Transformer-based time series forecaster with attention\"\"\"\n",
        "\n",
        "    def __init__(self, input_size, d_model, nhead, num_layers,\n",
        "                 dropout, forecast_horizon):\n",
        "        super(TransformerForecaster, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.forecast_horizon = forecast_horizon\n",
        "\n",
        "        # Input projection\n",
        "        self.input_projection = nn.Linear(input_size, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.positional_encoding = self._create_positional_encoding(500, d_model)\n",
        "\n",
        "        # Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
        "                                                  dim_feedforward=256,\n",
        "                                                  dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "\n",
        "        # Output layers\n",
        "        self.fc1 = nn.Linear(d_model, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(128, forecast_horizon)\n",
        "\n",
        "        self.attention_weights = None\n",
        "\n",
        "    def _create_positional_encoding(self, max_len, d_model):\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                            (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        return pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"x shape: (batch_size, seq_len, input_size)\"\"\"\n",
        "        seq_len = x.size(1)\n",
        "\n",
        "        x = self.input_projection(x)\n",
        "        x = x + self.positional_encoding[:, :seq_len, :].to(x.device)\n",
        "\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x[:, -1, :]  # Use last timestep\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# ============================================================================\n",
        "# PART 4: TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(X_batch)\n",
        "        loss = criterion(predictions, y_batch)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate model\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            predictions = model(X_batch)\n",
        "            loss = criterion(predictions, y_batch)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(val_loader)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs=100,\n",
        "                learning_rate=0.001, device='cpu', patience=15):\n",
        "    \"\"\"Full training loop with early stopping\"\"\"\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                     factor=0.5, patience=5,\n",
        "                                                     verbose=False)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        val_loss = validate(model, val_loader, criterion, device)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            break\n",
        "\n",
        "    return model\n",
        "\n",
        "# ============================================================================\n",
        "# PART 5: OPTUNA HYPERPARAMETER OPTIMIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def objective(trial, X_train, y_train, X_val, y_val, device, model_type='lstm'):\n",
        "    \"\"\"Objective function for Optuna optimization\"\"\"\n",
        "\n",
        "    # Suggest hyperparameters\n",
        "    if model_type == 'lstm':\n",
        "        hidden_size = trial.suggest_int('hidden_size', 32, 256, step=32)\n",
        "        num_layers = trial.suggest_int('num_layers', 1, 4)\n",
        "        dropout = trial.suggest_float('dropout', 0.1, 0.5, step=0.1)\n",
        "        learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
        "        batch_size = trial.suggest_int('batch_size', 16, 128, step=16)\n",
        "\n",
        "        model = LSTMForecaster(input_size=X_train.shape[2],\n",
        "                              hidden_size=hidden_size,\n",
        "                              num_layers=num_layers,\n",
        "                              dropout=dropout,\n",
        "                              forecast_horizon=y_train.shape[1])\n",
        "    else:\n",
        "        d_model = trial.suggest_int('d_model', 32, 128, step=32)\n",
        "        nhead = trial.suggest_int('nhead', 2, 8, step=2)\n",
        "        num_layers = trial.suggest_int('num_layers', 1, 3)\n",
        "        dropout = trial.suggest_float('dropout', 0.1, 0.5, step=0.1)\n",
        "        learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
        "        batch_size = trial.suggest_int('batch_size', 16, 128, step=16)\n",
        "\n",
        "        model = TransformerForecaster(input_size=X_train.shape[2],\n",
        "                                     d_model=d_model,\n",
        "                                     nhead=nhead,\n",
        "                                     num_layers=num_layers,\n",
        "                                     dropout=dropout,\n",
        "                                     forecast_horizon=y_train.shape[1])\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
        "    val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Train model\n",
        "    model = train_model(model, train_loader, val_loader, epochs=100,\n",
        "                       learning_rate=learning_rate, device=device, patience=10)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    model.eval()\n",
        "    val_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, _ in val_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            preds = model(X_batch).cpu().numpy()\n",
        "            val_predictions.append(preds)\n",
        "\n",
        "    val_predictions = np.concatenate(val_predictions)\n",
        "    val_mse = mean_squared_error(y_val, val_predictions)\n",
        "\n",
        "    return val_mse\n",
        "\n",
        "def optimize_hyperparameters(X_train, y_train, X_val, y_val,\n",
        "                            model_type='lstm', n_trials=20, device='cpu'):\n",
        "    \"\"\"Run Optuna optimization\"\"\"\n",
        "\n",
        "    sampler = optuna.samplers.TPESampler(seed=42)\n",
        "    pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
        "\n",
        "    study = optuna.create_study(sampler=sampler, pruner=pruner, direction='minimize')\n",
        "\n",
        "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val,\n",
        "                                          device, model_type),\n",
        "                  n_trials=n_trials, show_progress_bar=True)\n",
        "\n",
        "    print(f\"\\nBest trial value: {study.best_value:.6f}\")\n",
        "    print(f\"Best hyperparameters:\\n{study.best_params}\")\n",
        "\n",
        "    return study.best_params\n",
        "\n",
        "# ============================================================================\n",
        "# PART 6: STATISTICAL BASELINE (PROPHET)\n",
        "# ============================================================================\n",
        "\n",
        "def train_prophet_baseline(df, forecast_horizon=6):\n",
        "    \"\"\"Train Prophet model as statistical baseline\"\"\"\n",
        "    try:\n",
        "        from prophet import Prophet\n",
        "    except ImportError:\n",
        "        print(\"Prophet not installed. Skipping Prophet baseline.\")\n",
        "        return None\n",
        "\n",
        "    # Prepare data for Prophet\n",
        "    df_prophet = df.reset_index()\n",
        "    df_prophet.columns = ['ds', 'y']\n",
        "    df_prophet['ds'] = pd.to_datetime(df_prophet['ds'])\n",
        "\n",
        "    model = Prophet(yearly_seasonality=True,\n",
        "                   weekly_seasonality=True,\n",
        "                   daily_seasonality=True,\n",
        "                   seasonality_mode='additive',\n",
        "                   interval_width=0.95)\n",
        "\n",
        "    model.fit(df_prophet)\n",
        "\n",
        "    return model\n",
        "\n",
        "# ============================================================================\n",
        "# PART 7: EXPLAINABILITY WITH SHAP\n",
        "# ============================================================================\n",
        "\n",
        "def explain_predictions(model, X_test, feature_names, device, n_samples=100):\n",
        "    \"\"\"Generate SHAP explanations for model predictions\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Create wrapper function for SHAP\n",
        "    def predict_fn(X):\n",
        "        X_tensor = torch.FloatTensor(X).to(device)\n",
        "        with torch.no_grad():\n",
        "            return model(X_tensor).cpu().numpy()\n",
        "\n",
        "    # Use background and test data for SHAP\n",
        "    background_indices = np.random.choice(len(X_test), min(50, len(X_test)), replace=False)\n",
        "    X_background = X_test[background_indices]\n",
        "\n",
        "    # Flatten sequences for SHAP (samples, features)\n",
        "    X_background_flat = X_background.reshape(X_background.shape[0], -1)\n",
        "    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "    # Create SHAP explainer\n",
        "    explainer = shap.KernelExplainer(\n",
        "        lambda x: predict_fn(x.reshape(x.shape[0], X_test.shape[1], X_test.shape[2])),\n",
        "        X_background_flat\n",
        "    )\n",
        "\n",
        "    # Compute SHAP values for first n_samples\n",
        "    shap_values = explainer.shap_values(X_test_flat[:n_samples])\n",
        "\n",
        "    return explainer, shap_values, X_test_flat[:n_samples]\n",
        "\n",
        "# ============================================================================\n",
        "# PART 8: MAIN EXECUTION & ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\"*80)\n",
        "    print(\"Advanced Time Series Forecasting with Neural Networks\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Configuration\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"\\nUsing device: {DEVICE}\")\n",
        "\n",
        "    # ---- STEP 1: Generate Data ----\n",
        "    print(\"\\n[STEP 1] Generating synthetic multivariate time series...\")\n",
        "    df = generate_multivariate_timeseries(n_steps=2000, n_features=3, seed=42)\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "    print(f\"Features: {list(df.columns)}\")\n",
        "    print(f\"Date range: {df.index[0]} to {df.index[-1]}\")\n",
        "\n",
        "    # ---- STEP 2: Preprocessing ----\n",
        "    print(\"\\n[STEP 2] Preprocessing data...\")\n",
        "    preprocessor = TimeSeriesPreprocessor(lookback=24, forecast_horizon=6)\n",
        "    preprocessor.fit_scalers(df)\n",
        "\n",
        "    df_scaled = preprocessor.transform(df)\n",
        "    X, y = preprocessor.create_sequences(df_scaled, target_col=0)\n",
        "\n",
        "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = \\\n",
        "        preprocessor.train_val_test_split(X, y, train_ratio=0.7, val_ratio=0.15)\n",
        "\n",
        "    print(f\"Training set: X={X_train.shape}, y={y_train.shape}\")\n",
        "    print(f\"Validation set: X={X_val.shape}, y={y_val.shape}\")\n",
        "    print(f\"Test set: X={X_test.shape}, y={y_test.shape}\")\n",
        "\n",
        "    # ---- STEP 3: Hyperparameter Optimization ----\n",
        "    print(\"\\n[STEP 3] Hyperparameter optimization with Optuna (LSTM)...\")\n",
        "    best_params_lstm = optimize_hyperparameters(X_train, y_train, X_val, y_val,\n",
        "                                               model_type='lstm', n_trials=15,\n",
        "                                               device=DEVICE)\n",
        "\n",
        "    # ---- STEP 4: Train Final LSTM Model ----\n",
        "    print(\"\\n[STEP 4] Training optimized LSTM model...\")\n",
        "    lstm_model = LSTMForecaster(\n",
        "        input_size=X_train.shape[2],\n",
        "        hidden_size=best_params_lstm['hidden_size'],\n",
        "        num_layers=best_params_lstm['num_layers'],\n",
        "        dropout=best_params_lstm['dropout'],\n",
        "        forecast_horizon=y_train.shape[1]\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
        "    val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=best_params_lstm['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=best_params_lstm['batch_size'], shuffle=False)\n",
        "\n",
        "    lstm_model = train_model(lstm_model, train_loader, val_loader, epochs=100,\n",
        "                            learning_rate=best_params_lstm['learning_rate'],\n",
        "                            device=DEVICE, patience=15)\n",
        "\n",
        "    # ---- STEP 5: Evaluate LSTM ----\n",
        "    print(\"\\n[STEP 5] Evaluating models...\")\n",
        "    lstm_model.eval()\n",
        "    lstm_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, _ in DataLoader(TensorDataset(torch.FloatTensor(X_test)),\n",
        "                                     batch_size=32, shuffle=False):\n",
        "            X_batch = X_batch.to(DEVICE)\n",
        "            preds = lstm_model(X_batch).cpu().numpy()\n",
        "            lstm_predictions.append(preds)\n",
        "\n",
        "    lstm_predictions = np.concatenate(lstm_predictions)\n",
        "\n",
        "    # Convert predictions back to original scale\n",
        "    lstm_pred_original = preprocessor.inverse_transform(lstm_predictions.flatten(), 'feature_0')[:len(y_test)*6]\n",
        "    lstm_pred_original = lstm_pred_original.reshape(-1, 6)\n",
        "    y_test_original = preprocessor.inverse_transform(y_test.flatten(), 'feature_0')[:len(y_test)*6]\n",
        "    y_test_original = y_test_original.reshape(-1, 6)\n",
        "\n",
        "    lstm_rmse = np.sqrt(mean_squared_error(y_test_original, lstm_pred_original))\n",
        "    lstm_mae = mean_absolute_error(y_test_original, lstm_pred_original)\n",
        "    lstm_r2 = r2_score(y_test_original.flatten(), lstm_pred_original.flatten())\n",
        "\n",
        "    print(f\"\\nLSTM Model Performance:\")\n",
        "    print(f\"  RMSE: {lstm_rmse:.4f}\")\n",
        "    print(f\"  MAE:  {lstm_mae:.4f}\")\n",
        "    print(f\"  R²:   {lstm_r2:.4f}\")\n",
        "\n",
        "    # ---- STEP 6: Statistical Baseline ----\n",
        "    print(\"\\n[STEP 6] Training statistical baseline (SARIMAX)...\")\n",
        "    try:\n",
        "        # Use first feature for baseline\n",
        "        sarimax_model = SARIMAX(df['feature_0'].values,\n",
        "                               order=(1, 1, 1),\n",
        "                               seasonal_order=(1, 1, 1, 24),\n",
        "                               enforce_stationarity=False,\n",
        "                               enforce_invertibility=False).fit(disp=False)\n",
        "\n",
        "        # Forecast on test set\n",
        "        sarimax_predictions = sarimax_model.get_forecast(steps=len(y_test_original)*6).predicted_mean.values\n",
        "        sarimax_predictions = sarimax_predictions[:len(y_test_original)*6].reshape(-1, 6)\n",
        "\n",
        "        sarimax_rmse = np.sqrt(mean_squared_error(y_test_original, sarimax_predictions))\n",
        "        sarimax_mae = mean_absolute_error(y_test_original, sarimax_predictions)\n",
        "        sarimax_r2 = r2_score(y_test_original.flatten(), sarimax_predictions.flatten())\n",
        "\n",
        "        print(f\"\\nSARIMAX Model Performance:\")\n",
        "        print(f\"  RMSE: {sarimax_rmse:.4f}\")\n",
        "        print(f\"  MAE:  {sarimax_mae:.4f}\")\n",
        "        print(f\"  R²:   {sarimax_r2:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"SARIMAX training failed: {e}\")\n",
        "        sarimax_rmse = sarimax_mae = sarimax_r2 = None\n",
        "\n",
        "    # ---- STEP 7: Model Explainability ----\n",
        "    print(\"\\n[STEP 7] Generating SHAP explanations...\")\n",
        "    try:\n",
        "        explainer, shap_vals, X_test_flat = explain_predictions(\n",
        "            lstm_model, X_test,\n",
        "            [f'lag_{i}_feat_{j}' for i in range(24) for j in range(3)],\n",
        "            DEVICE, n_samples=50\n",
        "        )\n",
        "        print(f\"SHAP values computed for {len(shap_vals)} samples\")\n",
        "        print(f\"Average absolute SHAP value: {np.abs(shap_vals).mean():.6f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"SHAP explanation generation encountered issue: {e}\")\n",
        "\n",
        "    # ---- STEP 8: Visualization ----\n",
        "    print(\"\\n[STEP 8] Generating visualizations...\")\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Plot 1: Actual vs LSTM predictions\n",
        "    axes[0, 0].plot(y_test_original[:50, 0], label='Actual', linewidth=2)\n",
        "    axes[0, 0].plot(lstm_pred_original[:50, 0], label='LSTM', linestyle='--', linewidth=2)\n",
        "    axes[0, 0].set_title('LSTM: Actual vs Predicted (Step 1)')\n",
        "    axes[0, 0].set_xlabel('Test Sample')\n",
        "    axes[0, 0].set_ylabel('Value')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: LSTM residuals\n",
        "    residuals = y_test_original[:50, 0] - lstm_pred_original[:50, 0]\n",
        "    axes[0, 1].bar(range(len(residuals)), residuals, alpha=0.7, color='red')\n",
        "    axes[0, 1].axhline(y=0, color='k', linestyle='--')\n",
        "    axes[0, 1].set_title('LSTM Residuals (Step 1)')\n",
        "    axes[0, 1].set_xlabel('Test Sample')\n",
        "    axes[0, 1].set_ylabel('Residual')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 3: Performance comparison\n",
        "    if sarimax_rmse is not None:\n",
        "        models = ['LSTM', 'SARIMAX']\n",
        "        rmses = [lstm_rmse, sarimax_rmse]\n",
        "        colors = ['#2ecc71', '#e74c3c']\n",
        "    else:\n",
        "        models = ['LSTM']\n",
        "        rmses = [lstm_rmse]\n",
        "        colors = ['#2ecc71']\n",
        "\n",
        "    axes[1, 0].bar(models, rmses, color=colors, alpha=0.7)\n",
        "    axes[1, 0].set_title('RMSE Comparison')\n",
        "    axes[1, 0].set_ylabel('RMSE')\n",
        "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Plot 4: Forecast horizon performance\n",
        "    step_rmses = [np.sqrt(mean_squared_error(y_test_original[:, i], lstm_pred_original[:, i]))\n",
        "                  for i in range(6)]\n",
        "    axes[1, 1].plot(range(1, 7), step_rmses, marker='o', linewidth=2, markersize=8, color='#3498db')\n",
        "    axes[1, 1].set_title('RMSE by Forecast Step')\n",
        "    axes[1, 1].set_xlabel('Forecast Horizon (steps)')\n",
        "    axes[1, 1].set_ylabel('RMSE')\n",
        "    axes[1, 1].set_xticks(range(1, 7))\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('time_series_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"Visualization saved as 'time_series_analysis.png'\")\n",
        "\n",
        "    # ---- Summary Report ----\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ANALYSIS SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\"\"\n",
        "HYPERPARAMETER OPTIMIZATION RESULTS:\n",
        "  Model Type: LSTM\n",
        "  Best Parameters:\n",
        "    - Hidden Size: {best_params_lstm['hidden_size']}\n",
        "    - Number of Layers: {best_params_lstm['num_layers']}\n",
        "    - Dropout: {best_params_lstm['dropout']}\n",
        "    - Learning Rate: {best_params_lstm['learning_rate']:.6f}\n",
        "    - Batch Size: {best_params_lstm['batch_size']}\n",
        "\n",
        "DEEP LEARNING MODEL PERFORMANCE:\n",
        "  LSTM RMSE: {lstm_rmse:.4f}\n",
        "  LSTM MAE:  {lstm_mae:.4f}\n",
        "  LSTM R²:   {lstm_r2:.4f}\n",
        "\n",
        "STATISTICAL BASELINE PERFORMANCE:\n",
        "  SARIMAX RMSE: {sarimax_rmse:.4f if sarimax_rmse else 'N/A'}\n",
        "  SARIMAX MAE:  {sarimax_mae:.4f if sarimax_mae else 'N/A'}\n",
        "  SARIMAX R²:   {sarimax_r2:.4f if sarimax_r2 else 'N/A'}\n",
        "\n",
        "MODEL COMPARISON & INTERPRETATION:\n",
        "  Improvement (LSTM vs SARIMAX): {((sarimax_rmse - lstm_rmse)/sarimax_rmse*100):.2f}% if sarimax_rmse else 'N/A'} RMSE reduction\n",
        "\n",
        "FORECAST HORIZON ANALYSIS:\n",
        "  Step-wise RMSE: {[f'{r:.4f}' for r in step_rmses]}\n",
        "  Best performing step: {np.argmin(step_rmses) + 1}\n",
        "  Worst performing step: {np.argmax(step_rmses) + 1}\n",
        "\n",
        "EXPLAINABILITY INSIGHTS:\n",
        "  - SHAP analysis reveals which historical lags most influence predictions\n",
        "  - High-importance lags indicate strong temporal dependencies\n",
        "  - Model relies on multiple lookback windows for accurate forecasts\n",
        "  - Attention mechanisms capture non-linear temporal patterns\n",
        "    \"\"\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}